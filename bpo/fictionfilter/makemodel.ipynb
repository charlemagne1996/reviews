{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a fiction filter\n",
    "\n",
    "This very simply loads some training data and trains a regularized logistic regression on it, using gridsearch to find an optimal number of features and regularization constant. We optimize on F0.5 score, a harmonic mean of precision and recall that puts more emphasis on precision. In practice, I don't think this produces results hugely different from F1 score.\n",
    "\n",
    "There are more sophisticated feature-selection strategies than simply using *n* most common, but if I used more sophisticated selection strategies I would also need a more sophisticated validation strategy to avoid fooling myself; e.g. a validation set separate from the test set. Without extensive resources for generating labeled data, I'm trying to keep things relatively quick and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunder/miniconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequenceID</th>\n",
       "      <th>genrecode</th>\n",
       "      <th>#matchquality</th>\n",
       "      <th>#rareword</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>and</th>\n",
       "      <th>a</th>\n",
       "      <th>is</th>\n",
       "      <th>to</th>\n",
       "      <th>...</th>\n",
       "      <th>neither</th>\n",
       "      <th>self</th>\n",
       "      <th>rest</th>\n",
       "      <th>drawn</th>\n",
       "      <th>effect</th>\n",
       "      <th>somewhat</th>\n",
       "      <th>latter</th>\n",
       "      <th>lord</th>\n",
       "      <th>u</th>\n",
       "      <th>beauty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>393</td>\n",
       "      <td>n</td>\n",
       "      <td>2.351964</td>\n",
       "      <td>0.769964</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263</td>\n",
       "      <td>n</td>\n",
       "      <td>2.757639</td>\n",
       "      <td>0.733205</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>y</td>\n",
       "      <td>2.404113</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8-3</td>\n",
       "      <td>y</td>\n",
       "      <td>2.256296</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585</td>\n",
       "      <td>y</td>\n",
       "      <td>2.243333</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sequenceID genrecode  #matchquality  #rareword        of       the  \\\n",
       "0        393         n       2.351964   0.769964  0.001192  0.001192   \n",
       "1        263         n       2.757639   0.733205  0.000960  0.000960   \n",
       "2         56         y       2.404113   0.481013  0.004219  0.004219   \n",
       "3        8-3         y       2.256296   0.416667  0.016667  0.016667   \n",
       "4        585         y       2.243333   0.414894  0.010638  0.010638   \n",
       "\n",
       "        and         a        is        to   ...    neither     self  rest  \\\n",
       "0  0.001192  0.001192  0.001192  0.001192   ...    0.00000  0.00000   0.0   \n",
       "1  0.000960  0.000960  0.000960  0.000960   ...    0.00096  0.00096   0.0   \n",
       "2  0.004219  0.004219  0.004219  0.004219   ...    0.00000  0.00000   0.0   \n",
       "3  0.016667  0.016667  0.016667  0.016667   ...    0.00000  0.00000   0.0   \n",
       "4  0.010638  0.010638  0.010638  0.010638   ...    0.00000  0.00000   0.0   \n",
       "\n",
       "      drawn    effect  somewhat   latter      lord        u  beauty  \n",
       "0  0.001192  0.001192   0.00000  0.00000  0.001192  0.00000     0.0  \n",
       "1  0.000000  0.000000   0.00096  0.00096  0.000960  0.00096     0.0  \n",
       "2  0.000000  0.004219   0.00000  0.00000  0.000000  0.00000     0.0  \n",
       "3  0.000000  0.000000   0.00000  0.00000  0.000000  0.00000     0.0  \n",
       "4  0.000000  0.000000   0.00000  0.00000  0.000000  0.00000     0.0  \n",
       "\n",
       "[5 rows x 402 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = pd.read_csv('trainingdata.tsv', sep = '\\t')\n",
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the complete dataset as it exists on disk. We may not use all the features. Notice how the most common feature is '#rareword', i.e., English word outside this limited vocabulary.\n",
    "\n",
    "We're going to select only the columns for features, leaving out the first two metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(514, 400)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termdoc = rawdata.iloc[ : , 2 : 402]\n",
    "termdoc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a vector that maps genrecode (is it fiction n/y) to an integer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classvec = rawdata.genrecode.map({'n' : 0, 'y': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function that actually trains a logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onepass(cval, numfeatures, termdoc, classvec):\n",
    "    '''\n",
    "    cval is the regularization constant\n",
    "    numfeatures the number of features to use in the model\n",
    "    termdoc is X, aka the feature matrix\n",
    "    classvec is y, aka the vector of class integers to be predicted\n",
    "    '''\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(termdoc.iloc[ : , 0 : numfeatures])\n",
    "    \n",
    "    # Note that we scale and center the columns of the feature matrix.\n",
    "    \n",
    "    model = LogisticRegression(C = cval)\n",
    "    f1_scores = cross_val_score(model, data, classvec,\n",
    "                             scoring = 'f1', cv=10)\n",
    "    f1 = sum(f1_scores) / len(f1_scores)\n",
    "    # Tenfold crossvalidation, using F1 score.\n",
    "    \n",
    "    precision_scores = cross_val_score(model, data, classvec,\n",
    "                             scoring = 'precision', cv=10)\n",
    "    precision = sum(precision_scores) / len(precision_scores)\n",
    "    \n",
    "    recall_scores = cross_val_score(model, data, classvec,\n",
    "                             scoring = 'recall', cv=10)\n",
    "    recall = sum(recall_scores) / len(recall_scores)\n",
    "    \n",
    "    f05 = 1.5 * (precision * recall) / ((.5 * precision) + recall)\n",
    "    \n",
    "    model.fit(data, classvec)\n",
    "    predictions = model.predict(data)\n",
    "    \n",
    "    # We return both the average F1 score of a cross-\n",
    "    # validated model, and the predictions of a model\n",
    "    # trained on all the data.\n",
    "    \n",
    "    return f05, precision, recall, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search across a range of feature numbers and regularization constants. Notice that for regularization, we iterate across an integer range but then divide by ten thousand. So the best value of 100 is actually .01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8492405418 100 140\n",
      "0.847416426482 110 140\n",
      "0.850099335925 120 140\n",
      "0.849488127658 130 140\n",
      "0.850940640231 140 140\n",
      "0.850953064438 150 140\n",
      "0.848344292706 160 140\n",
      "0.849641291058 170 140\n",
      "0.846607974572 180 140\n",
      "0.846188518084 190 140\n",
      "0.847473999941 200 140\n",
      "0.848743070063 210 140\n",
      "0.850085351866 220 140\n",
      "0.85261777083 230 140\n",
      "0.853473000544 240 140\n",
      "0.854775704749 250 140\n",
      "0.851574026675 260 140\n",
      "0.849935311063 270 140\n",
      "0.849935311063 280 140\n",
      "0.849935311063 290 140\n",
      "0.851207614223 300 140\n",
      "0.851207614223 310 140\n",
      "0.851207614223 320 140\n",
      "0.852489834096 330 140\n",
      "0.852489834096 340 140\n",
      "0.855808583796 100 150\n",
      "0.857183175798 110 150\n",
      "0.854353873178 120 150\n",
      "0.854225643179 130 150\n",
      "0.855564658516 140 150\n",
      "0.857670900322 150 150\n",
      "0.857670900322 160 150\n",
      "0.858914859107 170 150\n",
      "0.860214972866 180 150\n",
      "0.861952001035 190 150\n",
      "0.864464332044 200 150\n",
      "0.862461831751 210 150\n",
      "0.863716861444 220 150\n",
      "0.863716861444 230 150\n",
      "0.864936831272 240 150\n",
      "0.864356934474 250 150\n",
      "0.865864767729 260 150\n",
      "0.865091222178 270 150\n",
      "0.865091222178 280 150\n",
      "0.865091222178 290 150\n",
      "0.863432499843 300 150\n",
      "0.863432499843 310 150\n",
      "0.864753404945 320 150\n",
      "0.867212131983 330 150\n",
      "0.866681891365 340 150\n",
      "0.858273523369 100 160\n",
      "0.860969167521 110 160\n",
      "0.859663708135 120 160\n",
      "0.858076924738 130 160\n",
      "0.858243354975 140 160\n",
      "0.863562725908 150 160\n",
      "0.86748077474 160 160\n",
      "0.862015454929 170 160\n",
      "0.862015454929 180 160\n",
      "0.863338371022 190 160\n",
      "0.863338371022 200 160\n",
      "0.864696759273 210 160\n",
      "0.864616793643 220 160\n",
      "0.863978218431 230 160\n",
      "0.861913825205 240 160\n",
      "0.861450484459 250 160\n",
      "0.861017621816 260 160\n",
      "0.862258387056 270 160\n",
      "0.860510785297 280 160\n",
      "0.860510785297 290 160\n",
      "0.863375329907 300 160\n",
      "0.863375329907 310 160\n",
      "0.863375329907 320 160\n",
      "0.864614253392 330 160\n",
      "0.865851696801 340 160\n",
      "0.857352613101 100 170\n",
      "0.860910135911 110 170\n",
      "0.863509252022 120 170\n",
      "0.864937882039 130 170\n",
      "0.860308007284 140 170\n",
      "0.860442986892 150 170\n",
      "0.861839701106 160 170\n",
      "0.864911943036 170 170\n",
      "0.861521050394 180 170\n",
      "0.859663137692 190 170\n",
      "0.860467139526 200 170\n",
      "0.860467139526 210 170\n",
      "0.860467139526 220 170\n",
      "0.858029363702 230 170\n",
      "0.859254139238 240 170\n",
      "0.860907270546 250 170\n",
      "0.859254139238 260 170\n",
      "0.860568987875 270 170\n",
      "0.85883569881 280 170\n",
      "0.860140721625 290 170\n",
      "0.861531837908 300 170\n",
      "0.859889540793 310 170\n",
      "0.859889540793 320 170\n",
      "0.861027779243 330 170\n",
      "0.861027779243 340 170\n",
      "0.859224037026 100 180\n",
      "0.859286435444 110 180\n",
      "0.861920756898 120 180\n",
      "0.862749597957 130 180\n",
      "0.864230493469 140 180\n",
      "0.864305081671 150 180\n",
      "0.865250032461 160 180\n",
      "0.863788875883 170 180\n",
      "0.863407087309 180 180\n",
      "0.860047535233 190 180\n",
      "0.858291406146 200 180\n",
      "0.854752728244 210 180\n",
      "0.856031385401 220 180\n",
      "0.858597801818 230 180\n",
      "0.855897544223 240 180\n",
      "0.854346893699 250 180\n",
      "0.855662824502 260 180\n",
      "0.853318526201 270 180\n",
      "0.853318526201 280 180\n",
      "0.854655151741 290 180\n",
      "0.856343701204 300 180\n",
      "0.856689030791 310 180\n",
      "0.855191218349 320 180\n",
      "0.856408966583 330 180\n",
      "0.8577343763 340 180\n",
      "0.848669373957 100 190\n",
      "0.843910691313 110 190\n",
      "0.849321471843 120 190\n",
      "0.852042337603 130 190\n",
      "0.851575321007 140 190\n",
      "0.854259646416 150 190\n",
      "0.852388305535 160 190\n",
      "0.857615432626 170 190\n",
      "0.858049337374 180 190\n",
      "0.856286274149 190 190\n",
      "0.854053653706 200 190\n",
      "0.854053653706 210 190\n",
      "0.854053653706 220 190\n",
      "0.852752261201 230 190\n",
      "0.852449503174 240 190\n",
      "0.852449503174 250 190\n",
      "0.850095385551 260 190\n",
      "0.852605581591 270 190\n",
      "0.852605581591 280 190\n",
      "0.852605581591 290 190\n",
      "0.853888953124 300 190\n",
      "0.853888953124 310 190\n",
      "0.853888953124 320 190\n",
      "0.853888953124 330 190\n",
      "0.851836909104 340 190\n",
      "0.856616224794 100 200\n",
      "0.857872185737 110 200\n",
      "0.856035484329 120 200\n",
      "0.85874710898 130 200\n",
      "0.862859255867 140 200\n",
      "0.865597135928 150 200\n",
      "0.86290995078 160 200\n",
      "0.859360686729 170 200\n",
      "0.860604881955 180 200\n",
      "0.860724589789 190 200\n",
      "0.858917247026 200 200\n",
      "0.858917247026 210 200\n",
      "0.858917247026 220 200\n",
      "0.86043533096 230 200\n",
      "0.861825277998 240 200\n",
      "0.861707102925 250 200\n",
      "0.865557126679 260 200\n",
      "0.859850708915 270 200\n",
      "0.859850708915 280 200\n",
      "0.861170664571 290 200\n",
      "0.863804354784 300 200\n",
      "0.863804354784 310 200\n",
      "0.863804354784 320 200\n",
      "0.862057639104 330 200\n",
      "0.861701584266 340 200\n",
      "0.853426708157 100 210\n",
      "0.85491713889 110 210\n",
      "0.85491713889 120 210\n",
      "0.856267566239 130 210\n",
      "0.858209663776 140 210\n",
      "0.858286942272 150 210\n",
      "0.858285883379 160 210\n",
      "0.858160131272 170 210\n",
      "0.8621528554 180 210\n",
      "0.8621528554 190 210\n",
      "0.86164789148 200 210\n",
      "0.86164789148 210 210\n",
      "0.864216719379 220 210\n",
      "0.86285555759 230 210\n",
      "0.86285555759 240 210\n",
      "0.862912644914 250 210\n",
      "0.862912644914 260 210\n",
      "0.864595329244 270 210\n",
      "0.864595329244 280 210\n",
      "0.867022026551 290 210\n",
      "0.869540588864 300 210\n",
      "0.872668093794 310 210\n",
      "0.873959777971 320 210\n",
      "0.870749008931 330 210\n",
      "0.867677942119 340 210\n",
      "0.857021098946 100 220\n",
      "0.855876923227 110 220\n",
      "0.852790201682 120 220\n",
      "0.852790201682 130 220\n",
      "0.854883292609 140 220\n",
      "0.855634078106 150 220\n",
      "0.855634078106 160 220\n",
      "0.857077659174 170 220\n",
      "0.858467153602 180 220\n",
      "0.858643541239 190 220\n",
      "0.861411157063 200 220\n",
      "0.860711319055 210 220\n",
      "0.861872474075 220 220\n",
      "0.863179087556 230 220\n",
      "0.863179087556 240 220\n",
      "0.864496880515 250 220\n",
      "0.866880104072 260 220\n",
      "0.86817498608 270 220\n",
      "0.86817498608 280 220\n",
      "0.869448353942 290 220\n",
      "0.871156951111 300 220\n",
      "0.871156951111 310 220\n",
      "0.871156951111 320 220\n",
      "0.871156951111 330 220\n",
      "0.870253045437 340 220\n",
      "0.866768795424 100 230\n",
      "0.854934440402 110 230\n",
      "0.854993999165 120 230\n",
      "0.857885726991 130 230\n",
      "0.859247848323 140 230\n",
      "0.86199254047 150 230\n",
      "0.862129005981 160 230\n",
      "0.858590804709 170 230\n",
      "0.86121443462 180 230\n",
      "0.86121443462 190 230\n",
      "0.862609747793 200 230\n",
      "0.866654819282 210 230\n",
      "0.868000751049 220 230\n",
      "0.869214942262 230 230\n",
      "0.869214942262 240 230\n",
      "0.871728737328 250 230\n",
      "0.870122412751 260 230\n",
      "0.871364210121 270 230\n",
      "0.871364210121 280 230\n",
      "0.871364210121 290 230\n",
      "0.872658746236 300 230\n",
      "0.872658746236 310 230\n",
      "0.87430593906 320 230\n",
      "0.87430593906 330 230\n",
      "0.873979489711 340 230\n",
      "0.865905801959 100 240\n",
      "0.864656564936 110 240\n",
      "0.860874653345 120 240\n",
      "0.857336753172 130 240\n",
      "0.861354184303 140 240\n",
      "0.861284608626 150 240\n",
      "0.864042982896 160 240\n",
      "0.86368352595 170 240\n",
      "0.86368352595 180 240\n",
      "0.863064627181 190 240\n",
      "0.864280183069 200 240\n",
      "0.864328515233 210 240\n",
      "0.864328515233 220 240\n",
      "0.864328515233 230 240\n",
      "0.864240377799 240 240\n",
      "0.864240377799 250 240\n",
      "0.863062359366 260 240\n",
      "0.866721779249 270 240\n",
      "0.868090906867 280 240\n",
      "0.868090906867 290 240\n",
      "0.869440027544 300 240\n",
      "0.870636596637 310 240\n",
      "0.868479489017 320 240\n",
      "0.869801172129 330 240\n",
      "0.869801172129 340 240\n",
      "0.868398210772 100 250\n",
      "0.865518163842 110 250\n",
      "0.86508686378 120 250\n",
      "0.865572536384 130 250\n",
      "0.865259762624 140 250\n",
      "0.864953331003 150 250\n",
      "0.863591809707 160 250\n",
      "0.863591809707 170 250\n",
      "0.861051804176 180 250\n",
      "0.861051804176 190 250\n",
      "0.862355415807 200 250\n",
      "0.865838851183 210 250\n",
      "0.862300999153 220 250\n",
      "0.861515746239 230 250\n",
      "0.861515746239 240 250\n",
      "0.86424565998 250 250\n",
      "0.862986528159 260 250\n",
      "0.862126435333 270 250\n",
      "0.863384015614 280 250\n",
      "0.864633894141 290 250\n",
      "0.864633894141 300 250\n",
      "0.864633894141 310 250\n",
      "0.867098261825 320 250\n",
      "0.867098261825 330 250\n",
      "0.868467614981 340 250\n",
      "0.858948585457 100 260\n",
      "0.860297813213 110 260\n",
      "0.860297813213 120 260\n",
      "0.860535978918 130 260\n",
      "0.857507270075 140 260\n",
      "0.856370513403 150 260\n",
      "0.860414100141 160 260\n",
      "0.863256164457 170 260\n",
      "0.860008120572 180 260\n",
      "0.863825418015 190 260\n",
      "0.863825418015 200 260\n",
      "0.865119217782 210 260\n",
      "0.863530061853 220 260\n",
      "0.862848304379 230 260\n",
      "0.861222739805 240 260\n",
      "0.858913330011 250 260\n",
      "0.863262357179 260 260\n",
      "0.863262357179 270 260\n",
      "0.866068269874 280 260\n",
      "0.864022069727 290 260\n",
      "0.864022069727 300 260\n",
      "0.86271085952 310 260\n",
      "0.86408248457 320 260\n",
      "0.861881827283 330 260\n",
      "0.863001606169 340 260\n",
      "0.857862746423 100 270\n",
      "0.857976074422 110 270\n",
      "0.859269496752 120 270\n",
      "0.858814312906 130 270\n",
      "0.85877190354 140 270\n",
      "0.86297564583 150 270\n",
      "0.859954745363 160 270\n",
      "0.863034167527 170 270\n",
      "0.865820218256 180 270\n",
      "0.86720867431 190 270\n",
      "0.865453468431 200 270\n",
      "0.863369751711 210 270\n",
      "0.861015593718 220 270\n",
      "0.862390132492 230 270\n",
      "0.862390132492 240 270\n",
      "0.862390132492 250 270\n",
      "0.86253949446 260 270\n",
      "0.86253949446 270 270\n",
      "0.861115330283 280 270\n",
      "0.861115330283 290 270\n",
      "0.861115330283 300 270\n",
      "0.861115330283 310 270\n",
      "0.860823074906 320 270\n",
      "0.862070715156 330 270\n",
      "0.865187968627 340 270\n",
      "0.860873492818 100 280\n",
      "0.85956058976 110 280\n",
      "0.861562782543 120 280\n",
      "0.864253476433 130 280\n",
      "0.861047473274 140 280\n",
      "0.866699140924 150 280\n",
      "0.870806970949 160 280\n",
      "0.870826772873 170 280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873594116549 180 280\n",
      "0.870152260062 190 280\n",
      "0.871516536179 200 280\n",
      "0.868176271068 210 280\n",
      "0.868176271068 220 280\n",
      "0.869964571279 230 280\n",
      "0.871261754041 240 280\n",
      "0.867413441514 250 280\n",
      "0.867413441514 260 280\n",
      "0.866061449432 270 280\n",
      "0.866061449432 280 280\n",
      "0.870147129385 290 280\n",
      "0.868803431525 300 280\n",
      "0.868803431525 310 280\n",
      "0.868803431525 320 280\n",
      "0.870124963723 330 280\n",
      "0.870124963723 340 280\n",
      "0.87474796975 100 290\n",
      "0.874842112922 110 290\n",
      "0.880953922062 120 290\n",
      "0.88046154544 130 290\n",
      "0.879165454661 140 290\n",
      "0.880481757443 150 290\n",
      "0.881910449235 160 290\n",
      "0.881910449235 170 290\n",
      "0.884573127037 180 290\n",
      "0.881510741405 190 290\n",
      "0.882801613137 200 290\n",
      "0.884094996539 210 290\n",
      "0.882801613137 220 290\n",
      "0.885407522842 230 290\n",
      "0.884069884319 240 290\n",
      "0.885407522842 250 290\n",
      "0.882614544851 260 290\n",
      "0.88144285139 270 290\n",
      "0.88144285139 280 290\n",
      "0.882206608578 290 290\n",
      "0.882206608578 300 290\n",
      "0.878343675407 310 290\n",
      "0.878343675407 320 290\n",
      "0.877033389997 330 290\n",
      "0.875302995576 340 290\n",
      "0.873993101547 100 300\n",
      "0.873993101547 110 300\n",
      "0.873644948233 120 300\n",
      "0.874987609218 130 300\n",
      "0.87631774394 140 300\n",
      "0.873560127387 150 300\n",
      "0.874854959111 160 300\n",
      "0.873491345591 170 300\n",
      "0.871304365284 180 300\n",
      "0.87285085069 190 300\n",
      "0.874213921252 200 300\n",
      "0.873009807475 210 300\n",
      "0.873009807475 220 300\n",
      "0.870028626527 230 300\n",
      "0.87275248317 240 300\n",
      "0.87275248317 250 300\n",
      "0.87275248317 260 300\n",
      "0.871427381045 270 300\n",
      "0.872764433479 280 300\n",
      "0.870683567109 290 300\n",
      "0.870683567109 300 300\n",
      "0.871842971794 310 300\n",
      "0.871481185024 320 300\n",
      "0.870174402611 330 300\n",
      "0.870174402611 340 300\n",
      "0.868374263626 100 310\n",
      "0.868336856742 110 310\n",
      "0.871020541006 120 310\n",
      "0.873708950228 130 310\n",
      "0.876398540381 140 310\n",
      "0.876398540381 150 310\n",
      "0.88043038217 160 310\n",
      "0.880047838023 170 310\n",
      "0.877888719503 180 310\n",
      "0.879156887048 190 310\n",
      "0.875262448659 200 310\n",
      "0.873241810659 210 310\n",
      "0.874560597633 220 310\n",
      "0.870744047224 230 310\n",
      "0.872059794986 240 310\n",
      "0.872059794986 250 310\n",
      "0.870436900713 260 310\n",
      "0.869057788896 270 310\n",
      "0.869057788896 280 310\n",
      "0.8689061894 290 310\n",
      "0.8689061894 300 310\n",
      "0.8689061894 310 310\n",
      "0.867481630968 320 310\n",
      "0.867481630968 330 310\n",
      "0.868752078002 340 310\n",
      "0.871990545242 100 320\n",
      "0.869700790598 110 320\n",
      "0.869730567757 120 320\n",
      "0.869730567757 130 320\n",
      "0.871132536595 140 320\n",
      "0.869905845946 150 320\n",
      "0.871220315138 160 320\n",
      "0.870899265345 170 320\n",
      "0.872262731029 180 320\n",
      "0.870955394738 190 320\n",
      "0.868793046118 200 320\n",
      "0.87013111953 210 320\n",
      "0.86791715827 220 320\n",
      "0.869289941651 230 320\n",
      "0.869289941651 240 320\n",
      "0.869289941651 250 320\n",
      "0.870645033895 260 320\n",
      "0.868631691447 270 320\n",
      "0.868348362663 280 320\n",
      "0.868348362663 290 320\n",
      "0.868348362663 300 320\n",
      "0.866986281162 310 320\n",
      "0.866986281162 320 320\n",
      "0.8655200828 330 320\n",
      "0.864945753084 340 320\n",
      "0.868542244388 100 330\n",
      "0.871332849316 110 330\n",
      "0.868610743917 120 330\n",
      "0.869690468864 130 330\n",
      "0.867938101537 140 330\n",
      "0.867538276194 150 330\n",
      "0.867538276194 160 330\n",
      "0.867297415416 170 330\n",
      "0.866769436729 180 330\n",
      "0.866292307544 190 330\n",
      "0.865032376443 200 330\n",
      "0.865032376443 210 330\n",
      "0.866292307544 220 330\n",
      "0.864788653086 230 330\n",
      "0.859586036248 240 330\n",
      "0.860972549325 250 330\n",
      "0.860972549325 260 330\n",
      "0.860972549325 270 330\n",
      "0.860551132337 280 330\n",
      "0.860551132337 290 330\n",
      "0.861846259834 300 330\n",
      "0.864670800883 310 330\n",
      "0.859884565106 320 330\n",
      "0.859860365278 330 330\n",
      "0.858577676356 340 330\n",
      "0.865727526436 100 340\n",
      "0.868480429865 110 340\n",
      "0.868080261122 120 340\n",
      "0.870689082687 130 340\n",
      "0.869301176886 140 340\n",
      "0.868304081941 150 340\n",
      "0.869561069214 160 340\n",
      "0.871198513214 170 340\n",
      "0.872554857215 180 340\n",
      "0.870420525639 190 340\n",
      "0.866587435958 200 340\n",
      "0.864511686952 210 340\n",
      "0.862690710008 220 340\n",
      "0.861403167063 230 340\n",
      "0.861045751449 240 340\n",
      "0.86051365448 250 340\n",
      "0.86023699006 260 340\n",
      "0.86023699006 270 340\n",
      "0.863013296413 280 340\n",
      "0.863013296413 290 340\n",
      "0.863013296413 300 340\n",
      "0.863013296413 310 340\n",
      "0.861805279179 320 340\n",
      "0.861805279179 330 340\n",
      "0.861805279179 340 340\n",
      "0.863671563847 100 350\n",
      "0.862399322037 110 350\n",
      "0.865255983913 120 350\n",
      "0.863800839093 130 350\n",
      "0.859957512754 140 350\n",
      "0.862728887929 150 350\n",
      "0.864039968797 160 350\n",
      "0.865190634189 170 350\n",
      "0.866026524128 180 350\n",
      "0.86335230645 190 350\n",
      "0.862786515348 200 350\n",
      "0.864058718208 210 350\n",
      "0.862399153389 220 350\n",
      "0.862399153389 230 350\n",
      "0.855206952956 240 350\n",
      "0.853922338938 250 350\n",
      "0.853922338938 260 350\n",
      "0.853922338938 270 350\n",
      "0.85268556063 280 350\n",
      "0.85268556063 290 350\n",
      "0.85268556063 300 350\n",
      "0.850754596521 310 350\n",
      "0.850754596521 320 350\n",
      "0.849203871827 330 350\n",
      "0.849203871827 340 350\n",
      "0.855773951354 100 360\n",
      "0.856980996058 110 360\n",
      "0.859785311997 120 360\n",
      "0.852325592963 130 360\n",
      "0.852325592963 140 360\n",
      "0.851411569132 150 360\n",
      "0.847706393463 160 360\n",
      "0.850499772979 170 360\n",
      "0.850499772979 180 360\n",
      "0.851817171803 190 360\n",
      "0.855543503915 200 360\n",
      "0.853596804377 210 360\n",
      "0.85492652951 220 360\n",
      "0.853258980877 230 360\n",
      "0.851633462992 240 360\n",
      "0.851612824239 250 360\n",
      "0.851612824239 260 360\n",
      "0.85292068989 270 360\n",
      "0.855621910926 280 360\n",
      "0.856946311969 290 360\n",
      "0.857223957058 300 360\n",
      "0.857223957058 310 360\n",
      "0.857223957058 320 360\n",
      "0.857223957058 330 360\n",
      "0.856750209094 340 360\n",
      "0.855953005223 100 370\n",
      "0.855996293109 110 370\n",
      "0.855996293109 120 370\n",
      "0.855935356119 130 370\n",
      "0.856025212637 140 370\n",
      "0.853674810196 150 370\n",
      "0.85378202963 160 370\n",
      "0.857934394264 170 370\n",
      "0.859650671705 180 370\n",
      "0.857340453914 190 370\n",
      "0.855201344032 200 370\n",
      "0.855350065141 210 370\n",
      "0.85356505973 220 370\n",
      "0.85356505973 230 370\n",
      "0.855999863349 240 370\n",
      "0.855999863349 250 370\n",
      "0.855999863349 260 370\n",
      "0.855999863349 270 370\n",
      "0.857254567913 280 370\n",
      "0.855925269418 290 370\n",
      "0.8525352564 300 370\n",
      "0.850533527993 310 370\n",
      "0.850533527993 320 370\n",
      "0.853263064105 330 370\n",
      "0.853263064105 340 370\n",
      "0.855507755005 100 380\n",
      "0.852734453938 110 380\n",
      "0.852715159783 120 380\n",
      "0.850991609526 130 380\n",
      "0.852333671586 140 380\n",
      "0.849593818263 150 380\n",
      "0.850032160901 160 380\n",
      "0.851478096898 170 380\n",
      "0.852916941013 180 380\n",
      "0.852916941013 190 380\n",
      "0.855749976283 200 380\n",
      "0.855792792434 210 380\n",
      "0.85549517159 220 380\n",
      "0.851616417482 230 380\n",
      "0.84890120292 240 380\n",
      "0.84890120292 250 380\n",
      "0.84890120292 260 380\n",
      "0.850301725775 270 380\n",
      "0.851943795648 280 380\n",
      "0.851943795648 290 380\n",
      "0.851943795648 300 380\n",
      "0.851943795648 310 380\n",
      "0.851858846361 320 380\n",
      "0.851858846361 330 380\n",
      "0.851858846361 340 380\n",
      "0.856607941364 100 390\n",
      "0.85239770508 110 390\n",
      "0.850976736 120 390\n",
      "0.850976736 130 390\n",
      "0.853647712905 140 390\n",
      "0.853272845873 150 390\n",
      "0.854719592756 160 390\n",
      "0.859121473107 170 390\n",
      "0.857288275033 180 390\n",
      "0.857288275033 190 390\n",
      "0.862751301241 200 390\n",
      "0.860533454043 210 390\n",
      "0.859567965754 220 390\n",
      "0.859567965754 230 390\n",
      "0.857405257351 240 390\n",
      "0.857405257351 250 390\n",
      "0.854755110539 260 390\n",
      "0.854755110539 270 390\n",
      "0.854755110539 280 390\n",
      "0.854572385313 290 390\n",
      "0.857450520803 300 390\n",
      "0.857450520803 310 390\n",
      "0.856054751934 320 390\n",
      "0.856054751934 330 390\n",
      "0.856054751934 340 390\n",
      "0.853938850283 100 400\n",
      "0.85262798996 110 400\n",
      "0.853932046418 120 400\n",
      "0.854237780435 130 400\n",
      "0.857104263785 140 400\n",
      "0.855494551359 150 400\n",
      "0.854075189799 160 400\n",
      "0.858194796826 170 400\n",
      "0.859620530521 180 400\n",
      "0.857273606927 190 400\n",
      "0.856467375214 200 400\n",
      "0.854053871936 210 400\n",
      "0.854032347244 220 400\n",
      "0.856698840372 230 400\n",
      "0.857996875354 240 400\n",
      "0.857996875354 250 400\n",
      "0.859816050688 260 400\n",
      "0.861175999771 270 400\n",
      "0.864543666253 280 400\n",
      "0.863147166623 290 400\n",
      "0.863147166623 300 400\n",
      "0.864543666253 310 400\n",
      "0.864543666253 320 400\n",
      "0.863196418956 330 400\n",
      "0.863196418956 340 400\n"
     ]
    }
   ],
   "source": [
    "bestscores = []\n",
    "for features in range(140, 410, 10):\n",
    "    for cval in range(100, 350, 10):\n",
    "        f05, precision, recall, predictions = onepass(cval/ 10000, features, termdoc, classvec)\n",
    "        print(f05, cval, features)\n",
    "        bestscores.append((f05, cval, features))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8854075228418341, 250, 290)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestscores.sort()\n",
    "bestscores[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best model more specifically. Note that precision is good, which is important.\n",
    "\n",
    "**Recall 78.8, precision 94.4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885407522842 0.943554839776 0.788253968254 337\n"
     ]
    }
   ],
   "source": [
    "f05, precision, recall, predictions = onepass(.025, 290, termdoc, classvec)\n",
    "print(f05, precision, recall, sum(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Produce a model to export\n",
    "\n",
    "Ultimately we have to make a model to use, and this can't be crossvalidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(termdoc.iloc[ : , 0 : 290])\n",
    "model = LogisticRegression(C = .0250)\n",
    "model.fit(data, classvec)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fictionreview_scaler.pkl', mode = 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('fictionreview_model.pkl', mode = 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "words = list(termdoc.columns)\n",
    "with open('fictionreview_vocab.txt', mode = 'w', encoding = 'utf-8') as f:\n",
    "    for w in words[0: 400]:\n",
    "        f.write(w + '\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we actually export it.\n",
    "\n",
    "#### Examine feature weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(termdoc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voyages -0.261827961886\n",
      "letters -0.224647052144\n",
      "subject -0.198401479471\n",
      "set -0.150085831867\n",
      "history -0.128063938661\n",
      "#rareword -0.126347714836\n",
      "every -0.125106509586\n",
      "under -0.121826325571\n",
      "i -0.118591572494\n",
      "those -0.117814165713\n",
      "years -0.113434810042\n",
      "must -0.112848852913\n",
      "have -0.0992388495922\n",
      "here -0.0985770540385\n",
      "think -0.0923195072766\n",
      "no -0.0916571324327\n",
      "my -0.0904910984421\n",
      "something -0.0821340851033\n",
      "whose -0.0788725257495\n",
      "even -0.0780926426055\n",
      "own -0.0773125400245\n",
      "#romannumeral -0.0755540278862\n",
      "j -0.0751430955746\n",
      "seems -0.0730622234412\n",
      "fact -0.0725249282783\n",
      "present -0.0720757908135\n",
      "among -0.071412565102\n",
      "#placename -0.0712506528482\n",
      "me -0.0695599992231\n",
      "who -0.0606753957184\n",
      "p -0.0606284239244\n",
      "each -0.0598917045514\n",
      "their -0.0593917753438\n",
      "general -0.0578274678573\n",
      "also -0.0574639018566\n",
      "there -0.05629184788\n",
      "natural -0.0562326352692\n",
      "mind -0.0553297658734\n",
      "h -0.0545785112665\n",
      "most -0.0536637550975\n",
      "t -0.052766847354\n",
      "co -0.049981536323\n",
      "till -0.0488838405709\n",
      "so -0.0477713800443\n",
      "cannot -0.0476778258289\n",
      "volume -0.0463702423687\n",
      "such -0.0459530017064\n",
      "few -0.0457712896953\n",
      "point -0.044124383947\n",
      "take -0.0437690047993\n",
      "when -0.0431939234895\n",
      "things -0.0427294241749\n",
      "while -0.042603528087\n",
      "g -0.0425497210616\n",
      "said -0.0422931384079\n",
      "we -0.0422072390953\n",
      "can -0.0420509544204\n",
      "place -0.0399859430257\n",
      "english -0.0397541447435\n",
      "nature -0.0397144069159\n",
      "thought -0.0396825132638\n",
      "others -0.0396210161084\n",
      "between -0.0395185225543\n",
      "come -0.0381721204994\n",
      "this -0.0361224436966\n",
      "name -0.0359221606995\n",
      "enough -0.0342208499595\n",
      "many -0.0342067574817\n",
      "book -0.03418766161\n",
      "taken -0.0338705907521\n",
      "might -0.0338665142955\n",
      "about -0.0333409461804\n",
      "being -0.0331612288022\n",
      "country -0.0309151139467\n",
      "spirit -0.0307447578287\n",
      "after -0.0301121075446\n",
      "writing -0.0299762726136\n",
      "head -0.0293138205081\n",
      "mr -0.0282901238948\n",
      "know -0.0273913488251\n",
      "these -0.0269847183492\n",
      "was -0.0264192601724\n",
      "least -0.0255331132341\n",
      "written -0.0247883904342\n",
      "against -0.0239320571585\n",
      "great -0.0231756647919\n",
      "kind -0.0231021913021\n",
      "character -0.0215220092261\n",
      "given -0.0214458789633\n",
      "w -0.0211952078632\n",
      "interesting -0.0204110381888\n",
      "people -0.0179713444066\n",
      "if -0.015776359655\n",
      "well -0.0156014527273\n",
      "day -0.0154106207453\n",
      "to -0.0148269058552\n",
      "words -0.0138469920854\n",
      "which -0.0133523207279\n",
      "f -0.0131192165058\n",
      "because -0.0125416190165\n",
      "o -0.0120567788717\n",
      "by -0.0102784218289\n",
      "the -0.00989345231907\n",
      "should -0.00955761763723\n",
      "time -0.00935174212512\n",
      "means -0.00894346987814\n",
      "were -0.00870969862725\n",
      "put -0.00792796848512\n",
      "first -0.00718388209989\n",
      "d -0.00684092551988\n",
      "us -0.00582124284109\n",
      "world -0.00537269242117\n",
      "however -0.00480750655879\n",
      "and -0.00460416264775\n",
      "will -0.00346597077716\n",
      "be -0.00317790521607\n",
      "work -0.0031140380323\n",
      "house -0.00285549708357\n",
      "now -0.00255881465499\n",
      "whom -0.00131656798863\n",
      "often -0.000450489152503\n",
      "of 0.000798866642059\n",
      "nor 0.00147460995073\n",
      "but 0.00191096610498\n",
      "give 0.0021093166476\n",
      "full 0.00406613225214\n",
      "that 0.00460551359307\n",
      "has 0.00472713744021\n",
      "far 0.00510933227881\n",
      "without 0.00539312304281\n",
      "a 0.00572343962674\n",
      "would 0.00669544709242\n",
      "it 0.00820896315142\n",
      "as 0.00862349502948\n",
      "whole 0.00930714591872\n",
      "all 0.00931959131076\n",
      "seen 0.0103309613874\n",
      "itself 0.0103529781577\n",
      "#personalname 0.0107823288194\n",
      "old 0.0108009581202\n",
      "long 0.0109427681719\n",
      "been 0.0110945663828\n",
      "told 0.0110952476982\n",
      "almost 0.0113368530121\n",
      "still 0.0117495024159\n",
      "once 0.0120937606949\n",
      "then 0.0125112410245\n",
      "s 0.0129866161975\n",
      "b 0.0133788437701\n",
      "new 0.0143663538951\n",
      "indeed 0.0145195960783\n",
      "heart 0.015413848938\n",
      "either 0.0158646663054\n",
      "never 0.016810297536\n",
      "always 0.0178270458064\n",
      "at 0.0178827268992\n",
      "had 0.0183121809883\n",
      "them 0.0183236164442\n",
      "having 0.0184540625583\n",
      "better 0.0186666982375\n",
      "is 0.0195816493376\n",
      "m 0.0199417425885\n",
      "did 0.0199510989767\n",
      "found 0.0221869793803\n",
      "true 0.0232634991554\n",
      "himself 0.0239547991519\n",
      "made 0.0269485246103\n",
      "again 0.0276623584338\n",
      "where 0.0278469142206\n",
      "out 0.0283373260251\n",
      "look 0.0302357262835\n",
      "make 0.0322537907218\n",
      "with 0.0329000766992\n",
      "form 0.0335597691276\n",
      "called 0.0335969314745\n",
      "known 0.0337820830999\n",
      "another 0.0338808647331\n",
      "power 0.034951847687\n",
      "what 0.0360710207896\n",
      "men 0.0364120446852\n",
      "could 0.0366801745835\n",
      "from 0.0367653303694\n",
      "back 0.037826664982\n",
      "author 0.0378949671106\n",
      "than 0.0383918581143\n",
      "very 0.0387363395156\n",
      "left 0.0400972359321\n",
      "upon 0.0415370648003\n",
      "end 0.0416063740908\n",
      "both 0.0419523596702\n",
      "books 0.0421600116628\n",
      "his 0.0425857832601\n",
      "go 0.0427828933484\n",
      "are 0.042833693144\n",
      "through 0.0429258440269\n",
      "part 0.0431790108732\n",
      "nothing 0.0434078088167\n",
      "its 0.0435320543993\n",
      "before 0.0436160545685\n",
      "r 0.0437794046957\n",
      "#notenglishword 0.0448562034971\n",
      "an 0.0458145252918\n",
      "last 0.0465571225445\n",
      "half 0.0486553461572\n",
      "she 0.050051303916\n",
      "may 0.050888282974\n",
      "though 0.0515012804998\n",
      "life 0.0523640856681\n",
      "case 0.053181305667\n",
      "away 0.0536988286976\n",
      "interest 0.0557531492692\n",
      "same 0.0557676315879\n",
      "perhaps 0.0559178784343\n",
      "just 0.0567874251121\n",
      "good 0.0575637846159\n",
      "days 0.0581188955805\n",
      "some 0.0583552714722\n",
      "over 0.0587818006617\n",
      "into 0.0590874832724\n",
      "done 0.0599427126588\n",
      "find 0.0600262299143\n",
      "do 0.0603392286099\n",
      "little 0.0611040931782\n",
      "three 0.0619864811844\n",
      "see 0.0620187245616\n",
      "death 0.0621944125589\n",
      "her 0.0624752684752\n",
      "style 0.0637191447462\n",
      "e 0.0642642847677\n",
      "mrs 0.0645203141716\n",
      "up 0.0648751914756\n",
      "on 0.0652539051167\n",
      "thus 0.0652929776006\n",
      "one 0.0653855175385\n",
      "rather 0.0654733063476\n",
      "n 0.0664872420741\n",
      "or 0.0670454962761\n",
      "certain 0.0701106652223\n",
      "course 0.0706158295861\n",
      "tales 0.0715775882901\n",
      "writer 0.0725786127979\n",
      "more 0.0736594345261\n",
      "woman 0.0737444178548\n",
      "hand 0.0748975368637\n",
      "for 0.0755180765032\n",
      "real 0.0759256705545\n",
      "lady 0.0768562993467\n",
      "two 0.0772286839453\n",
      "other 0.0779632557228\n",
      "read 0.0786437773572\n",
      "any 0.0790789231796\n",
      "reader 0.0794333136297\n",
      "themselves 0.0803922159875\n",
      "wife 0.0818219747136\n",
      "in 0.0849111687947\n",
      "they 0.0867193124542\n",
      "human 0.0884210148645\n",
      "yet 0.0885830052363\n",
      "like 0.0892715547875\n",
      "man 0.0894367134156\n",
      "not 0.0979671398593\n",
      "him 0.103667068526\n",
      "much 0.103713444677\n",
      "ever 0.107130294202\n",
      "quite 0.108181665328\n",
      "way 0.110690769751\n",
      "only 0.11113338879\n",
      "he 0.114386075975\n",
      "young 0.116461830537\n",
      "best 0.116721669364\n",
      "father 0.121292810159\n",
      "vols 0.122368126819\n",
      "how 0.122872582183\n",
      "say 0.123941988756\n",
      "less 0.124459581495\n",
      "too 0.128781466127\n",
      "does 0.130113060291\n",
      "off 0.130811917397\n",
      "love 0.134453002357\n",
      "down 0.136089290025\n",
      "tale 0.136875327577\n",
      "our 0.143627696531\n",
      "characters 0.159101175627\n",
      "readers 0.172747826684\n",
      "hero 0.196168443264\n",
      "story 0.217246253015\n",
      "#matchquality 0.224209205432\n",
      "you 0.239441142398\n",
      "novels 0.259908333227\n",
      "novel 0.291005723434\n"
     ]
    }
   ],
   "source": [
    "features = list(zip(list(model.coef_[0]), words[0: 290]))\n",
    "features.sort()\n",
    "for x, y in features:\n",
    "    print(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
