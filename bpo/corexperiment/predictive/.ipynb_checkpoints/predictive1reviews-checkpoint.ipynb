{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive modeling of subject/genre categories\n",
    "\n",
    "Code to infer the strength of the boundary that separates a category of fiction from the rest of the works in our matched dataset. While we're treating the dataset as \"background\" here, we don't mean to imply that it's a vanilla or absolutely random sample. On the contrary, these are all works that were collected by academic libraries and reviewed by at least one periodical, so very obscure works of fiction (or those with an ephemeral popular audience) are likely to have been excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, ast, string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuationstring = string.punctuation + '—‘’“”'\n",
    "punctzapper = str.maketrans(punctuationstring, ' ' * len(punctuationstring))\n",
    "\n",
    "bpodict = dict()\n",
    "\n",
    "with open('../../filtered/all_fic_reviews.txt', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        bpoid = int(fields[0])\n",
    "\n",
    "        cleaned_text = fields[2].translate(punctzapper)\n",
    "        bpodict[bpoid] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vols_4_category(metapath, books, texts, classids, thisclass):\n",
    "    global bpodict\n",
    "    \n",
    "    metadf = pd.read_csv(metapath, sep = '\\t', index_col = 'bookid')\n",
    "    \n",
    "    for bookid, row in metadf.iterrows():\n",
    "        thetext = []\n",
    "        bpoids = ast.literal_eval(row['bpoids'])\n",
    "\n",
    "        for b in bpoids:\n",
    "            thetext.append(bpodict[b])\n",
    "            \n",
    "        if len(thetext) > 0:\n",
    "            thetext = ' '.join(thetext)\n",
    "            books.append(bookid)\n",
    "            texts.append(thetext)\n",
    "            classids.append(thisclass)\n",
    "                   \n",
    "def get_genretexts(gname):\n",
    "    books = []\n",
    "    texts = []\n",
    "    classids = []\n",
    "    \n",
    "    root = '../../genremeta/'\n",
    "    mainpath = root + gname + '.tsv'\n",
    "    get_vols_4_category(mainpath, books, texts, classids, 1)\n",
    "    \n",
    "    contrastpath = root + gname + '_contrast.tsv'\n",
    "    get_vols_4_category(contrastpath, books, texts, classids, 0)\n",
    "    \n",
    "    genredf = pd.DataFrame({'classid': classids, 'text': texts}, index = books)\n",
    "    \n",
    "    return genredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(genredf):\n",
    "    positive = genredf.loc[genredf['classid'] == 1, : ]\n",
    "    negative = genredf.loc[genredf['classid'] == 0, : ]\n",
    "    \n",
    "    postrain = positive.sample(n = 100)\n",
    "    negtrain = negative.sample(n = 100)\n",
    "    \n",
    "    # Here we should really have some code to ensure that authors\n",
    "    # are represented in train OR in test. But this is a first pass.\n",
    "    \n",
    "    traindf = pd.concat([postrain, negtrain])\n",
    "    \n",
    "    takenids = traindf.index.tolist()\n",
    "    \n",
    "    allids = genredf.index.tolist()\n",
    "    remainingids = set(allids) - set(takenids)\n",
    "    \n",
    "    testdf = genredf.loc[remainingids, : ]\n",
    "    return traindf.sample(frac = 1), testdf.sample(frac = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(traindf):\n",
    "    \n",
    "    # first count words in all the texts\n",
    "    vectorizer = CountVectorizer(max_features = 10000).fit(traindf['text'])\n",
    "    \n",
    "    # this returns a sparse matrix which will need transformation\n",
    "    sparse_matrix = vectorizer.transform(traindf['text'])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # we don't really have to name the columns, but we do need to transform\n",
    "    # the sparse matrix to a normal one\n",
    "    termdoc = pd.DataFrame(sparse_matrix.toarray(), columns = feature_names)\n",
    "    \n",
    "    # normalize by dividing by the number of words in each text\n",
    "    rowsums = termdoc.sum(axis=1)\n",
    "    normalized_termdoc = termdoc.div(rowsums, axis = 0)\n",
    "    \n",
    "    colsums = normalized_termdoc.sum(axis = 0)\n",
    "    \n",
    "    # also scale by the mean and stdev of each feature\n",
    "    scaler = StandardScaler().fit(normalized_termdoc)\n",
    "    scaled_matrix = scaler.transform(normalized_termdoc)\n",
    "    \n",
    "    # get features sorted by mutual information\n",
    "    # feature_mi = mutual_info_classif(scaled_matrix, traindf['classid'])\n",
    "    feature_tuples = [x for x in zip(colsums, vectorizer.get_feature_names())]\n",
    "    feature_tuples.sort(reverse = True)\n",
    "    \n",
    "    featureindices = []\n",
    "    for freq, feature in feature_tuples:\n",
    "        featureindices.append(feature_names.index(feature))\n",
    "    \n",
    "    y = traindf['classid']\n",
    "    \n",
    "    # now we're prepared for gridsearch\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for featurelen in [10, 50, 100, 200, 400, 800, 1600, 2400, 3200]:\n",
    "        for C in [.0001, .001, .01, .1, 1, 10, 100, 1000]:\n",
    "            \n",
    "            X = scaled_matrix[ : , featureindices[0 : featurelen]]\n",
    "            modeler = LogisticRegression(C = C)\n",
    "            scores = cross_validate(modeler, X, y, cv = 4, n_jobs = 6)\n",
    "            accuracy = np.mean(scores['test_score'])\n",
    "            results.append((accuracy, 1/featurelen, featurelen, C))\n",
    "            \n",
    "            # the reason for 1/featurelen is that in cases of an accuracy tie\n",
    "            # I want the sorting to choose the model with the smallest feature\n",
    "            # count!\n",
    "    \n",
    "    results.sort(reverse = True)\n",
    "    \n",
    "    acc, inversetosort, featurelen, C = results[0]\n",
    "    \n",
    "    X = scaled_matrix[ : , featureindices[0 : featurelen]]\n",
    "    best_model = LogisticRegression(C = C).fit(X, y)\n",
    "    \n",
    "    return results[0: 20], featureindices[0 : featurelen], vectorizer, scaler, best_model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_testset(testdf, features, vectorizer, scaler, best_model):\n",
    "    \n",
    "    sparse_matrix = vectorizer.transform(testdf['text'])\n",
    "    termdoc = pd.DataFrame(sparse_matrix.toarray())\n",
    "    \n",
    "    # normalize by dividing by the number of words in each text\n",
    "    rowsums = termdoc.sum(axis=1)\n",
    "    normalized_termdoc = termdoc.div(rowsums, axis = 0)\n",
    "    \n",
    "    # and scale using the saved scaler\n",
    "    scaled_matrix = scaler.transform(normalized_termdoc)\n",
    "    X = scaled_matrix[ : , features]\n",
    "    y = testdf['classid']\n",
    "    \n",
    "    predictions = best_model.predict(X)\n",
    "    \n",
    "    testacc = sum(predictions == y) / len(y)\n",
    "    print('test accuracy: ', testacc, testdf.shape[0])\n",
    "    print()\n",
    "    \n",
    "    return testacc\n",
    "    \n",
    "\n",
    "def repeatedly_validate(gname):\n",
    "    \n",
    "    print(gname)\n",
    "    print()\n",
    "    \n",
    "    genredf = get_genretexts(gname)\n",
    "    traintrials = []\n",
    "    validations = []\n",
    "    featurelens = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        train, test = test_train_split(genredf)\n",
    "        \n",
    "        results, features, vectorizer, scaler, best_model = grid_search(train)\n",
    "        \n",
    "        acc, inversetosort, featurelen, C = results[0]\n",
    "        print(gname + ' cross-validation acc: ', acc, featurelen, C)\n",
    "        traintrials.append(acc)\n",
    "        featurelens.append(featurelen)\n",
    "        \n",
    "        validacc = apply_to_testset(test, features, vectorizer, scaler, best_model)\n",
    "        validations.append(validacc)\n",
    "    \n",
    "    return np.mean(validations), validations, np.mean(traintrials), np.mean(featurelens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_genres(genre_names):\n",
    "\n",
    "    with open('predictivereviewresults1.tsv', mode = 'w', encoding = 'utf-8') as f:\n",
    "        f.write('genre\\tmeanvalidation\\tvalidations\\tmean_cv\\tmean_features\\n')\n",
    "        \n",
    "    for gname in genre_names:\n",
    "        meanvalid, validations, mean_cv, mean_features = repeatedly_validate(gname)\n",
    "        \n",
    "        with open('predictivereviewresults1.tsv', mode = 'a', encoding = 'utf-8') as f:\n",
    "            f.write(gname + '\\t' + str(meanvalid) + '\\t' + str(validations) + '\\t' + str(mean_cv) +\n",
    "                    '\\t' + str(mean_features) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biography\n",
      "\n",
      "biography cross-validation acc:  0.6599999999999999 1600 0.01\n",
      "test accuracy:  0.6533333333333333 150\n",
      "\n",
      "biography cross-validation acc:  0.655 2400 0.0001\n",
      "test accuracy:  0.64 150\n",
      "\n",
      "biography cross-validation acc:  0.6000000000000001 400 0.001\n",
      "test accuracy:  0.5866666666666667 150\n",
      "\n",
      "britain\n",
      "\n",
      "britain cross-validation acc:  0.6000000000000001 50 0.01\n",
      "test accuracy:  0.5652173913043478 138\n",
      "\n",
      "britain cross-validation acc:  0.625 100 1\n",
      "test accuracy:  0.4927536231884058 138\n",
      "\n",
      "britain cross-validation acc:  0.5700000000000001 800 0.0001\n",
      "test accuracy:  0.5797101449275363 138\n",
      "\n",
      "englishfiction\n",
      "\n",
      "englishfiction cross-validation acc:  0.55 800 1000\n",
      "test accuracy:  0.545 200\n",
      "\n",
      "englishfiction cross-validation acc:  0.5900000000000001 1600 0.001\n",
      "test accuracy:  0.485 200\n",
      "\n",
      "englishfiction cross-validation acc:  0.61 10 0.0001\n",
      "test accuracy:  0.5 200\n",
      "\n",
      "folklore\n",
      "\n",
      "folklore cross-validation acc:  0.9349999999999999 2400 100\n",
      "test accuracy:  0.9 80\n",
      "\n",
      "folklore cross-validation acc:  0.9199999999999999 2400 0.01\n",
      "test accuracy:  0.875 80\n",
      "\n",
      "folklore cross-validation acc:  0.915 800 0.001\n",
      "test accuracy:  0.9 80\n",
      "\n",
      "history\n",
      "\n",
      "history cross-validation acc:  0.7000000000000001 800 0.001\n",
      "test accuracy:  0.5632183908045977 174\n",
      "\n",
      "history cross-validation acc:  0.6599999999999999 2400 0.001\n",
      "test accuracy:  0.6551724137931034 174\n",
      "\n",
      "history cross-validation acc:  0.6950000000000001 800 1000\n",
      "test accuracy:  0.5919540229885057 174\n",
      "\n",
      "juvenile\n",
      "\n",
      "juvenile cross-validation acc:  0.815 800 0.01\n",
      "test accuracy:  0.725 200\n",
      "\n",
      "juvenile cross-validation acc:  0.77 800 0.01\n",
      "test accuracy:  0.765 200\n",
      "\n",
      "juvenile cross-validation acc:  0.7850000000000001 3200 0.001\n",
      "test accuracy:  0.745 200\n",
      "\n",
      "northamerica\n",
      "\n",
      "northamerica cross-validation acc:  0.5650000000000001 50 1000\n",
      "test accuracy:  0.4797979797979798 198\n",
      "\n",
      "northamerica cross-validation acc:  0.5950000000000001 200 0.01\n",
      "test accuracy:  0.5202020202020202 198\n",
      "\n",
      "northamerica cross-validation acc:  0.585 200 0.001\n",
      "test accuracy:  0.5202020202020202 198\n",
      "\n",
      "novel\n",
      "\n",
      "novel cross-validation acc:  0.5900000000000001 200 0.0001\n",
      "test accuracy:  0.62 200\n",
      "\n",
      "novel cross-validation acc:  0.625 400 0.001\n",
      "test accuracy:  0.58 200\n",
      "\n",
      "novel cross-validation acc:  0.64 2400 1000\n",
      "test accuracy:  0.485 200\n",
      "\n",
      "random\n",
      "\n",
      "random cross-validation acc:  0.5650000000000001 1600 0.0001\n",
      "test accuracy:  0.46632124352331605 193\n",
      "\n",
      "random cross-validation acc:  0.5650000000000001 10 0.001\n",
      "test accuracy:  0.5357142857142857 196\n",
      "\n",
      "random cross-validation acc:  0.5800000000000001 800 0.001\n",
      "test accuracy:  0.4896907216494845 194\n",
      "\n",
      "romance\n",
      "\n",
      "romance cross-validation acc:  0.77 100 1\n",
      "test accuracy:  0.6272727272727273 110\n",
      "\n",
      "romance cross-validation acc:  0.7350000000000001 50 0.1\n",
      "test accuracy:  0.6454545454545455 110\n",
      "\n",
      "romance cross-validation acc:  0.74 50 100\n",
      "test accuracy:  0.7454545454545455 110\n",
      "\n",
      "social\n",
      "\n",
      "social cross-validation acc:  0.605 50 0.001\n",
      "test accuracy:  0.4857142857142857 70\n",
      "\n",
      "social cross-validation acc:  0.5850000000000001 50 0.1\n",
      "test accuracy:  0.4714285714285714 70\n",
      "\n",
      "social cross-validation acc:  0.645 3200 0.001\n",
      "test accuracy:  0.5285714285714286 70\n",
      "\n",
      "stories\n",
      "\n",
      "stories cross-validation acc:  0.8300000000000001 200 0.1\n",
      "test accuracy:  0.735 200\n",
      "\n",
      "stories cross-validation acc:  0.78 800 0.0001\n",
      "test accuracy:  0.675 200\n",
      "\n",
      "stories cross-validation acc:  0.82 1600 0.001\n",
      "test accuracy:  0.685 200\n",
      "\n",
      "unmarked\n",
      "\n",
      "unmarked cross-validation acc:  0.61 10 0.001\n",
      "test accuracy:  0.41708542713567837 199\n",
      "\n",
      "unmarked cross-validation acc:  0.52 100 10\n",
      "test accuracy:  0.5175879396984925 199\n",
      "\n",
      "unmarked cross-validation acc:  0.55 200 0.01\n",
      "test accuracy:  0.5326633165829145 199\n",
      "\n",
      "war\n",
      "\n",
      "war cross-validation acc:  0.7050000000000001 100 1\n",
      "test accuracy:  0.6551724137931034 116\n",
      "\n",
      "war cross-validation acc:  0.7600000000000001 1600 0.0001\n",
      "test accuracy:  0.7068965517241379 116\n",
      "\n",
      "war cross-validation acc:  0.7450000000000001 800 0.001\n",
      "test accuracy:  0.6637931034482759 116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre_names = ['biography', 'britain', 'englishfiction', 'folklore', 'history', 'juvenile', 'northamerica', \n",
    "               'novel', 'random', 'romance', 'social', 'stories', 'unmarked', 'war']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try_all_genres(genre_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
