{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive modeling of subject/genre categories\n",
    "\n",
    "Code to infer the strength of the boundary that separates a category of fiction from the rest of the works in our matched dataset. While we're treating the dataset as \"background\" here, we don't mean to imply that it's a vanilla or absolutely random sample. On the contrary, these are all works that were collected by academic libraries and reviewed by at least one periodical, so very obscure works of fiction (or those with an ephemeral popular audience) are likely to have been excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, ast\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vols_4_category(metapath, books, texts, classids, thisclass):\n",
    "    metadf = pd.read_csv(metapath, sep = '\\t', index_col = 'bookid')\n",
    "    \n",
    "    for bookid, row in metadf.iterrows():\n",
    "        htrcids = ast.literal_eval(row['htrcids'])\n",
    "\n",
    "        thetext = []\n",
    "    \n",
    "        for h in htrcids:\n",
    "            inpath = '/Users/tunder/data/corexdata/' + h + '.txt'\n",
    "            if os.path.exists(inpath):\n",
    "                with open(inpath) as f:\n",
    "                    thisdoc = f.read()\n",
    "                    thetext.append(thisdoc)\n",
    "            else: \n",
    "                print('File error:', h)\n",
    "            \n",
    "        if len(thetext) > 0:\n",
    "            thetext = ' '.join(thetext)\n",
    "            books.append(bookid)\n",
    "            texts.append(thetext)\n",
    "            classids.append(thisclass)\n",
    "            \n",
    "def get_genretexts(gname):\n",
    "    books = []\n",
    "    texts = []\n",
    "    classids = []\n",
    "    \n",
    "    root = '../../genremeta/'\n",
    "    mainpath = root + gname + '.tsv'\n",
    "    get_vols_4_category(mainpath, books, texts, classids, 1)\n",
    "    \n",
    "    contrastpath = root + gname + '_contrast.tsv'\n",
    "    get_vols_4_category(contrastpath, books, texts, classids, 0)\n",
    "    \n",
    "    genredf = pd.DataFrame({'classid': classids, 'text': texts}, index = books)\n",
    "    \n",
    "    return genredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(genredf):\n",
    "    positive = genredf.loc[genredf['classid'] == 1, : ]\n",
    "    negative = genredf.loc[genredf['classid'] == 0, : ]\n",
    "    \n",
    "    postrain = positive.sample(n = 100)\n",
    "    negtrain = negative.sample(n = 100)\n",
    "    \n",
    "    # Here we should really have some code to ensure that authors\n",
    "    # are represented in train OR in test. But this is a first pass.\n",
    "    \n",
    "    traindf = pd.concat([postrain, negtrain])\n",
    "    \n",
    "    takenids = traindf.index.tolist()\n",
    "    \n",
    "    allids = genredf.index.tolist()\n",
    "    remainingids = set(allids) - set(takenids)\n",
    "    \n",
    "    testdf = genredf.loc[remainingids, : ]\n",
    "    return traindf.sample(frac = 1), testdf.sample(frac = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(traindf):\n",
    "    \n",
    "    # first count words in all the texts\n",
    "    vectorizer = CountVectorizer(max_features = 10000).fit(traindf['text'])\n",
    "    \n",
    "    # this returns a sparse matrix which will need transformation\n",
    "    sparse_matrix = vectorizer.transform(traindf['text'])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # we don't really have to name the columns, but we do need to transform\n",
    "    # the sparse matrix to a normal one\n",
    "    termdoc = pd.DataFrame(sparse_matrix.toarray(), columns = feature_names)\n",
    "    \n",
    "    # normalize by dividing by the number of words in each text\n",
    "    rowsums = termdoc.sum(axis=1)\n",
    "    normalized_termdoc = termdoc.div(rowsums, axis = 0)\n",
    "    \n",
    "    # also scale by the mean and stdev of each feature\n",
    "    scaler = StandardScaler().fit(normalized_termdoc)\n",
    "    scaled_matrix = scaler.transform(normalized_termdoc)\n",
    "    \n",
    "    # get features sorted by mutual information\n",
    "    feature_mi = mutual_info_classif(scaled_matrix, traindf['classid'])\n",
    "    feature_tuples = [x for x in zip(feature_mi, vectorizer.get_feature_names())]\n",
    "    feature_tuples.sort(reverse = True)\n",
    "    \n",
    "    featureindices = []\n",
    "    for mi, feature in feature_tuples:\n",
    "        featureindices.append(feature_names.index(feature))\n",
    "    \n",
    "    y = traindf['classid']\n",
    "    \n",
    "    # now we're prepared for gridsearch\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for featurelen in [100, 200, 400, 800, 1600, 2400, 3200, 4000, 6000]:\n",
    "        for C in [.00001, .0001, .001, .01, .1, 1, 10, 100]:\n",
    "            \n",
    "            X = scaled_matrix[ : , featureindices[0 : featurelen]]\n",
    "            modeler = LogisticRegression(C = C)\n",
    "            scores = cross_validate(modeler, X, y, cv = 10, n_jobs = 4)\n",
    "            accuracy = np.mean(scores['test_score'])\n",
    "            results.append((accuracy, 1/featurelen, featurelen, C))\n",
    "            \n",
    "            # the reason for 1/featurelen is that in cases of an accuracy tie\n",
    "            # I want the sorting to choose the model with the smallest feature\n",
    "            # count!\n",
    "    \n",
    "    results.sort(reverse = True)\n",
    "    \n",
    "    acc, inversetosort, featurelen, C = results[0]\n",
    "    \n",
    "    X = scaled_matrix[ : , featureindices[0 : featurelen]]\n",
    "    best_model = LogisticRegression(C = C).fit(X, y)\n",
    "    \n",
    "    return results[0: 20], featureindices[0 : featurelen], vectorizer, scaler, best_model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_testset(testdf, features, vectorizer, scaler, best_model):\n",
    "    \n",
    "    sparse_matrix = vectorizer.transform(testdf['text'])\n",
    "    termdoc = pd.DataFrame(sparse_matrix.toarray())\n",
    "    \n",
    "    # normalize by dividing by the number of words in each text\n",
    "    rowsums = termdoc.sum(axis=1)\n",
    "    normalized_termdoc = termdoc.div(rowsums, axis = 0)\n",
    "    \n",
    "    # and scale using the saved scaler\n",
    "    scaled_matrix = scaler.transform(normalized_termdoc)\n",
    "    X = scaled_matrix[ : , features]\n",
    "    y = testdf['classid']\n",
    "    \n",
    "    predictions = best_model.predict(X)\n",
    "    \n",
    "    testacc = sum(predictions == y) / len(y)\n",
    "    print('test accuracy: ', testacc, testdf.shape[0])\n",
    "    print()\n",
    "    \n",
    "    return testacc\n",
    "    \n",
    "\n",
    "def repeatedly_validate(gname):\n",
    "    \n",
    "    print(gname)\n",
    "    print()\n",
    "    \n",
    "    genredf = get_genretexts(gname)\n",
    "    traintrials = []\n",
    "    validations = []\n",
    "    featurelens = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        train, test = test_train_split(genredf)\n",
    "        \n",
    "        results, features, vectorizer, scaler, best_model = grid_search(train)\n",
    "        \n",
    "        acc, inversetosort, featurelen, C = results[0]\n",
    "        print(gname + ' cross-validation acc: ', acc, featurelen, C)\n",
    "        traintrials.append(acc)\n",
    "        featurelens.append(featurelen)\n",
    "        \n",
    "        validacc = apply_to_testset(test, features, vectorizer, scaler, best_model)\n",
    "        validations.append(validacc)\n",
    "    \n",
    "    return np.mean(validations), validations, np.mean(traintrials), np.mean(featurelens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_genres(genre_names):\n",
    "\n",
    "    with open('predictive1results.tsv', mode = 'w', encoding = 'utf-8') as f:\n",
    "        f.write('genre\\tmeanvalidation\\tvalidations\\tmean_cv\\tmean_features\\n')\n",
    "        \n",
    "    for gname in genre_names:\n",
    "        meanvalid, validations, mean_cv, mean_features = repeatedly_validate(gname)\n",
    "        \n",
    "        with open('predictive1results.tsv', mode = 'a', encoding = 'utf-8') as f:\n",
    "            f.write(gname + '\\t' + str(meanvalid) + '\\t' + str(validations) + '\\t' + str(mean_cv) +\n",
    "                    '\\t' + str(mean_features) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biography\n",
      "\n",
      "biography cross-validation acc:  0.7849999999999999 400 0.1\n",
      "test accuracy:  0.6 150\n",
      "\n",
      "biography cross-validation acc:  0.77 400 0.1\n",
      "test accuracy:  0.6733333333333333 150\n",
      "\n",
      "biography cross-validation acc:  0.775 200 0.01\n",
      "test accuracy:  0.6733333333333333 150\n",
      "\n",
      "britain\n",
      "\n",
      "britain cross-validation acc:  0.825 2400 10\n",
      "test accuracy:  0.6811594202898551 138\n",
      "\n",
      "britain cross-validation acc:  0.8200000000000001 2400 10\n",
      "test accuracy:  0.7463768115942029 138\n",
      "\n",
      "britain cross-validation acc:  0.78 400 0.0001\n",
      "test accuracy:  0.5869565217391305 138\n",
      "\n",
      "englishfiction\n",
      "\n",
      "englishfiction cross-validation acc:  0.7200000000000001 800 0.01\n",
      "test accuracy:  0.61 200\n",
      "\n",
      "englishfiction cross-validation acc:  0.72 1600 0.1\n",
      "test accuracy:  0.615 200\n",
      "\n",
      "englishfiction cross-validation acc:  0.71 800 0.001\n",
      "test accuracy:  0.575 200\n",
      "\n",
      "folklore\n",
      "\n",
      "folklore cross-validation acc:  0.9499999999999998 400 0.1\n",
      "test accuracy:  0.9375 80\n",
      "\n",
      "folklore cross-validation acc:  0.975 400 0.01\n",
      "test accuracy:  0.9 80\n",
      "\n",
      "folklore cross-validation acc:  0.9550000000000001 400 0.01\n",
      "test accuracy:  0.95 80\n",
      "\n",
      "history\n",
      "\n",
      "File error: nyp.33433044068033\n",
      "history cross-validation acc:  0.8400000000000001 3200 100\n",
      "test accuracy:  0.7745664739884393 173\n",
      "\n",
      "history cross-validation acc:  0.825 3200 100\n",
      "test accuracy:  0.7687861271676301 173\n",
      "\n",
      "history cross-validation acc:  0.8450000000000001 6000 100\n",
      "test accuracy:  0.7341040462427746 173\n",
      "\n",
      "juvenile\n",
      "\n",
      "juvenile cross-validation acc:  0.9099999999999999 800 0.01\n",
      "test accuracy:  0.83 200\n",
      "\n",
      "juvenile cross-validation acc:  0.875 2400 10\n",
      "test accuracy:  0.845 200\n",
      "\n",
      "juvenile cross-validation acc:  0.885 400 0.1\n",
      "test accuracy:  0.775 200\n",
      "\n",
      "northamerica\n",
      "\n",
      "northamerica cross-validation acc:  0.8150000000000001 2400 100\n",
      "test accuracy:  0.6161616161616161 198\n",
      "\n",
      "northamerica cross-validation acc:  0.8150000000000001 2400 100\n",
      "test accuracy:  0.6464646464646465 198\n",
      "\n",
      "northamerica cross-validation acc:  0.7550000000000001 2400 0.01\n",
      "test accuracy:  0.6818181818181818 198\n",
      "\n",
      "novel\n",
      "\n",
      "novel cross-validation acc:  0.79 3200 10\n",
      "test accuracy:  0.585 200\n",
      "\n",
      "novel cross-validation acc:  0.735 2400 100\n",
      "test accuracy:  0.565 200\n",
      "\n",
      "novel cross-validation acc:  0.6799999999999999 2400 10\n",
      "test accuracy:  0.67 200\n",
      "\n",
      "random\n",
      "\n",
      "File error: nyp.33433044068033\n",
      "random cross-validation acc:  0.66 800 0.1\n",
      "test accuracy:  0.4845360824742268 194\n",
      "\n",
      "random cross-validation acc:  0.735 200 0.01\n",
      "test accuracy:  0.47668393782383417 193\n",
      "\n",
      "random cross-validation acc:  0.695 200 0.1\n",
      "test accuracy:  0.5794871794871795 195\n",
      "\n",
      "romance\n",
      "\n",
      "romance cross-validation acc:  0.745 3200 0.01\n",
      "test accuracy:  0.7 110\n",
      "\n",
      "romance cross-validation acc:  0.77 400 0.01\n",
      "test accuracy:  0.6727272727272727 110\n",
      "\n",
      "romance cross-validation acc:  0.76 4000 0.01\n",
      "test accuracy:  0.6909090909090909 110\n",
      "\n",
      "social\n",
      "\n",
      "social cross-validation acc:  0.79 1600 1\n",
      "test accuracy:  0.6714285714285714 70\n",
      "\n",
      "social cross-validation acc:  0.78 2400 100\n",
      "test accuracy:  0.7142857142857143 70\n",
      "\n",
      "social cross-validation acc:  0.8000000000000002 800 0.1\n",
      "test accuracy:  0.5714285714285714 70\n",
      "\n",
      "stories\n",
      "\n",
      "stories cross-validation acc:  0.8300000000000001 1600 0.1\n",
      "test accuracy:  0.655 200\n",
      "\n",
      "stories cross-validation acc:  0.835 3200 1\n",
      "test accuracy:  0.725 200\n",
      "\n",
      "stories cross-validation acc:  0.8099999999999999 400 0.1\n",
      "test accuracy:  0.64 200\n",
      "\n",
      "unmarked\n",
      "\n",
      "unmarked cross-validation acc:  0.65 1600 0.01\n",
      "test accuracy:  0.5778894472361809 199\n",
      "\n",
      "unmarked cross-validation acc:  0.7300000000000001 800 0.1\n",
      "test accuracy:  0.5577889447236181 199\n",
      "\n",
      "unmarked cross-validation acc:  0.7000000000000001 1600 10\n",
      "test accuracy:  0.51 200\n",
      "\n",
      "war\n",
      "\n",
      "war cross-validation acc:  0.885 200 0.1\n",
      "test accuracy:  0.8362068965517241 116\n",
      "\n",
      "war cross-validation acc:  0.905 2400 100\n",
      "test accuracy:  0.7672413793103449 116\n",
      "\n",
      "war cross-validation acc:  0.89 200 100\n",
      "test accuracy:  0.8275862068965517 116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre_names = ['biography', 'britain', 'englishfiction', 'folklore', 'history', 'juvenile', 'northamerica', \n",
    "               'novel', 'random', 'romance', 'social', 'stories', 'unmarked', 'war']\n",
    "\n",
    "try_all_genres(genre_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
